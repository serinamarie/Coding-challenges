# READ IN MULTIPLE CSVS from another directory or same directory and concatenate
    # import pandas and glob
    # all_csvs = glob.glob('/folder/*.csv')
    # all_dfs = (pd.read_csv(f) for f in all_csvs)
    # concatted_df = pd.concat(all_dfs, ignore_index=True)

    # alternative
    # df = pd.concat(map(pd.read_csv, glob.glob('./folder/*.csv')))

    # with csv (import csv)
        # with open('/folder/flights.csv') as f:
            # d = csv.DictReader(f)
            # lol = list(d)

    # with pandas
        # with header row
            # pd.read_csv('data.csv')
        # with header in second row 
            # pd.read_csv("data.csv", header=1)
        # write your own column names in place of header
            # pd.read_csv('data.csv', skiprows=1, names=["better_name1", "better_name2"])
        # skip rows but keep header 
            # pd.read_csv('data.csv', skiprows=[1,2]) <-- skips 2nd and 3rd row!!!
        # without header 
            # pd.read_csv('data.csv', header=None)
        # set index column
            # pd.read_csv('data.csv', index_col="ID")
        # only the first n rows / last n rows
            # pd.read_csv('data.csv', nrows=5)
            # pd.read_csv('data,csv', skip_footer=10)
        # only specific columns 
            # pd.read_csv('data.csv', usecols=[1,5,7])
        # delimiter semi-colon
            # pd.read_csv('data.csv', sep=';') working alias: delimiter
        # measure time taken to import  
            # verbose = True 
        # change column type while importing
            # pd.read_csv('data.csv', dtype={'cost': 'float64'})
    
# CSV TO DATABASE TABLE W/ NO WRANGLING
    # with open('students.csv') as f:
        # csv_data = csv.reader(f)
    # with conn.cursor() as cur:
        # for row in csv_data:
            # cur.execute('INSERT INTO student(id, first_name, last_name)' 'VALUES(%s, %s, %s), row)


# WRANGLE DF WITH PANDAS
    # filter by column value, e.g. with df.query('origin == "PHX" & dest == "MCO"')
        # also could use .loc or df subset
    # filter by row and column with .iloc[r,w] 
        # df.iloc[1:5] for 2nd - 5th row
        # df.iloc[5,0] for 6th row and 1st column
        # df.iloc[2:7,1:3] for 3rd - 7th row, 2nd-3rd column
    # filter for multiple values in a column
        # df[df.origin.isin(["JFK", "MCO"])]
    # negate a condition
        # df[~((df.origin == "PHX") & (df.dest == "MCO"))]
    # no nulls
        # df[df['origin'].notnull()] 
    # filter strings 
        # df[df['origin'].str.len()>3]
        # df[df['origin'].str.contains('A'|'B')]
    # rename columns
        # df.rename(columns={'origin': 'orig'}, inplace=True)
# WRANGLE DATA NO DF/PANDAS
    # lambda_method = list(filter(lambda x: x['origin'] == 'PHX' and x['dest'] == 'MCO', lol))
    # list_comp_method = list(x for x in lol if x['origin'] == 'PHX' and x['dest'] == "MCO')

# connect to db with pyodbc or sqlite or something
    # import pyodbc 
    # conn = pyodbc.connect()

    # import mysql.connector
    # from mysql.connector import Error
    # conn = mysql.connector.connect(host=, user=, passwd=)
    # can use exceptions with Error, instead of having to write them out
        # except Error as err:
            # print(f"Error: '{err}'")

    # import psycopg2 
    # conn = psycopg2.connect(host=, database=, user=, password=)

    # import sqlite3
    # from sqlite3 import Error
    # conn = sqlite3.connect('test.db')
    # try, except Error:
        # print(Error)
    

# create a db
    # with sqlalchemy
        # conn.execute("CREATE DATABASE test")

    # with pyodbc 
        # cur.execute("CREATE DATABASE [TestDatabase]")

    # with mysql
        # cur.execute("CREATE DATABASE testdatabase")
        # additionally
            # cur.execute('SHOW DATABASES')

    # with pymongo
        # from pymongo import MongoClient
        # client = MongoClient('localhost', 27017)
        # db = client['testdb']
        # additionally  
            # print(client.list_database_names())

    # with psycopg2 
        # conn = psycopg2.connect(database=, user=, password, host=, port=)
        # curs.execute("""CREATE DATABASE test""")

# create a table
    create_table = '''CREATE TABLE test(
        id serial PRIMARY KEY,
        num INT,
        data VARCHAR
    );
    '''

# insert data 
    # manually
        # '''
        INSERT INTO products values(
            "baseball cap", "10", "red"),
            ("journal", "15", "blue"); 
            '''
    # from csv.. after creating a table
        # mysql
        '''
        LOAD DATA LOCAL INFILE 'folder/test.csv'
        INTO TABLE student
        FIELD TERMINATED BY ','
        LINES TERMINATED BY '\n'
        IGNORE 1 rows
        (id, first_name, last_name)'
        '''
        # postgresql
        ''' COPY test_table(id, first_name, last_name)
        FROM '/folder/data.csv'
        DELIMITER ','
        CSV HEADER;
        '''
        # returns COPY n, where n is the number of rows in the table
    # from df 
        # WITH SQLALCHEMY
            # from sqlalchemy import create_engine
            # engine = create_engine('sqlite://', echo=False)
            # df.to_sql('users', con=engine, if_exists='replace', index=False)
                # sql --> df --> csv OR json
                    # df2 = pd.read_sql("select * from users", con=engine)
                    # df2.to_csv('df2.csv') or df2.to_json(orient=)
            # WITHOUT SQLALCHEMY 
                # df.to_sql('users', con=conn, if_exists='replace', index = False)
    # from json
        # with open('*.json') as f:
            data = json.load(f)
            
        # if file contents,
            # data = json.loads('content')

# query db  
    # cur.execute(query).fetchall()
    # cur.fetchall()


# save data as CSV or JSON in current directory, or in different directory 
    # check the current directory and save to current or another
        # os.getcwd()
        # list_df.to_csv('very_important.csv', index=False)
        
        # OR os.chdir('/folder/wheredatagoes') first

        # result.to_json('output.json', orient=)
        # return json.dumps(result)