------INPUT-------

# READ IN MULTIPLE CSVS from another directory or same directory and concatenate
    import pandas and glob
    all_csvs = glob.glob('/folder/*.csv')
    all_dfs = (pd.read_csv(f) for f in all_csvs)
    concatted_df = pd.concat(all_dfs, ignore_index=True)

    alternative
    df = pd.concat(map(pd.read_csv, glob.glob('./folder/*.csv')))

    with csv (import csv)
        with open('/folder/flights.csv') as f:
            d = csv.DictReader(f)
            lol = list(d)
            
    with pandas
        with header row
            pd.read_csv('data.csv')
        with header in second row 
            pd.read_csv("data.csv", header=1)
        write your own column names in place of header
            pd.read_csv('data.csv', skiprows=1, names=["better_name1", "better_name2"])
        skip rows but keep header 
            pd.read_csv('data.csv', skiprows=[1,2]) <-- skips 2nd and 3rd row!!!
        without header 
            pd.read_csv('data.csv', header=None)
        set index column
            pd.read_csv('data.csv', index_col="ID")
        only the first n rows / last n rows
            pd.read_csv('data.csv', nrows=5)
            pd.read_csv('data,csv', skip_footer=10)
        only specific columns 
            pd.read_csv('data.csv', usecols=[1,5,7])
        delimiter semi-colon
            pd.read_csv('data.csv', sep=';') working alias: delimiter
        measure time taken to import  
            verbose = True 
        change column type while importing
            pd.read_csv('data.csv', dtype={'cost': 'float64'})
            
# WRANGLE DF WITH PANDAS
    # filter by column value, e.g. with df.query('origin == "PHX" & dest == "MCO"')
        # also could use .loc or df subset
    # filter by row and column with .iloc[r,w] 
        # df.iloc[1:5] for 2nd - 5th row
        # df.iloc[5,0] for 6th row and 1st column
        # df.iloc[2:7,1:3] for 3rd - 7th row, 2nd-3rd column
    # filter for multiple values in a column
        # df[df.origin.isin(["JFK", "MCO"])]
    # update an existing column's values if a certain condition is met 
        # df.loc[df['numbers'] < 100, 'numbers'] = 0
    # negate a condition
        # df[~((df.origin == "PHX") & (df.dest == "MCO"))]
    # no nulls
        # df[df['origin'].notnull()] 
    # filter strings 
        # df[df['origin'].str.len()>3]
        # df[df['origin'].str.contains('A'|'B')]
    # rename columns
        # df.rename(columns={'origin': 'orig'}, inplace=True)
# WRANGLE DATA NO DF/PANDAS
    # lambda_method = list(filter(lambda x: x['origin'] == 'PHX' and x['dest'] == 'MCO', lol))
    # list_comp_method = list(x for x in lol if x['origin'] == 'PHX' and x['dest'] == "MCO')
            
------CREATE-------

# CREATE A CONNECTION AND A DB IF NOT EXISTS
    import sqlite3
    from sqlite3 import Error
    conn = sqlite3.connect('test.db')
    try, except Error:
        print(Error)
        
    # with mysql
        import mysql.connector
        conn = mysql.connector.connect(host=, user=, password=, database=)
        can use exceptions with Error, instead of having to write them out
        except Error as err:
            print(f"Error: '{err}'")
        cur.execute("CREATE DATABASE testdatabase")
        additionally
            cur.execute('SHOW DATABASES')
            
    # with psycopg2 
        conn = psycopg2.connect(database=, user=, password, host=, port=)
        cur.execute("""CREATE DATABASE test""")

    # with sqlalchemy
        from sqlalchemy import create_engine
        engine = create_engine('sqlite://', echo=False)
        conn.execute("CREATE DATABASE test")

    # with pyodbc 
        import pyodbc 
        conn = pyodbc.connect()
        cur.execute("CREATE DATABASE [TestDatabase]")
        
    # with pymongo
        from pymongo import MongoClient
        client = MongoClient('localhost', 27017)
        db = client['testdb']
        # additionally  
            print(client.list_database_names())

# CREATE A TABLE
    create_table = '''CREATE TABLE test(
        id serial PRIMARY KEY,
        num INT,
        data VARCHAR(3)
    );'''
    
# ALTER TABLE
    alter_table_query = '''ALTER TABLE table_name
        ADD last_name VARCHAR(30);'''
        
    change_datatype_query = '''ALTER TABLE table_name
        MODIFY COLUMN column_name datatype;'''

------READ INTO DB-------

# CSV TO DB 
    with open('students.csv') as f:
        csv_data = csv.reader(f)
    with conn.cursor() as cur:
        for row in csv_data:
            cur.execute('INSERT INTO student(id, first_name, last_name)' 'VALUES(%s, %s, %s), row)

# MANUAL INSERTION
    '''
    INSERT INTO products values(
        "baseball cap", "10", "red"),
        ("journal", "15", "blue"); 
        '''
        
# DATAFRAME TO DB
    df.to_sql('users', con=conn, if_exists='replace', index = False)
    
# INSERT JSON (by transforming into Python dict)
    with open('*.json') as f:
        record_list = json.load(f)

    # or if string
    record_list = json.loads('content')
     
    for record in record_list:
        with conn.cursor() as cur:
            query = (
            "INSERT INTO EMPLOYEE(FIRST_NAME, LAST_NAME, AGE, SEX, INCOME)"
            "VALUES (%s, %s, %s, %s, %s)"
            )
            data = ('Ramya', 'Ramapriya', 25, 'F', 5000)
            cursor.execute(insert_stmt, data)
            conn.commit()

    for record in record_list:
        with conn.cursor() as cur:
            query = 'UPDATE summary SET totalconfirmed=%s WHERE country=%s;'
            data = (record['TotalConfirmed'], record['Country'])
            cur.execute(query, data)
            conn.commit()
     return 'Summary table updated'
 
------READ FROM DB-------

# cur.execute(query).fetchall()
# or cur.fetchall() after cur.execute(query)

# df = pd.read_sql("select * from users", con=engine)
# df = pd.read_sql("""
    SELECT first_name, age
    FROM people
    WHERE age > 10
    ORDER BY first_name
    """, 
    con = conn)








------OUTPUT-------

# SAVE DATA AS CSV or JSON in current directory, or in different directory 
    # check the current directory and save to current or another
        # os.getcwd() OR os.chdir('/folder/wheredatagoes') 
        # df.to_csv('very_important.csv', index=False)
   
        # df.to_json('*.json', orient=)
        # result.to_json('output.json', orient=)
        # return json.dumps(result)
        
------EXTRAS-------

# INSERT DATA DIRECTLY FROM A CSV, AFTER CREATING A TABLE
    # mysql
    '''
    LOAD DATA LOCAL INFILE 'folder/test.csv'
    INTO TABLE student
    FIELD TERMINATED BY ','
    LINES TERMINATED BY '\n'
    IGNORE 1 rows
    (id, first_name, last_name)'
    '''
    # postgresql
    ''' COPY test_table(id, first_name, last_name)
    FROM '/folder/data.csv'
    DELIMITER ','
    CSV HEADER;
    '''
    # returns COPY n, where n is the number of rows in the table
    

# USE DICTIONARY TO UPDATE EXISTING TABLE IN SQLITE3
    for record in record_list:

        with conn.cursor() as cur:
            sql = 'UPDATE summary SET date=?, totalconfirmed=? WHERE country=?;'
            data = (record['Date'], record['TotalConfirmed'], record['Country'])
            cur.execute(sql, data)
            conn.commit()
